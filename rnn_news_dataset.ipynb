{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWarning: 1.5 GB download.\\nDownload the bag of words (from Google News) from this link: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Warning: 1.5 GB download.\n",
    "Download the bag of words (from Google News) from this link: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monster/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MX_NB_WORDS = 200000\n",
    "MAX_SEQ_LEN = 256\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: \t From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------------------------------------\n",
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peek on the data\n",
    "print('Data: \\t', \"\\n---------------------------------------------------\\n\".join(twenty_train.data[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'alt.atheism'),\n",
       " (1, 'comp.graphics'),\n",
       " (2, 'comp.os.ms-windows.misc'),\n",
       " (3, 'comp.sys.ibm.pc.hardware'),\n",
       " (4, 'comp.sys.mac.hardware'),\n",
       " (5, 'comp.windows.x'),\n",
       " (6, 'misc.forsale'),\n",
       " (7, 'rec.autos'),\n",
       " (8, 'rec.motorcycles'),\n",
       " (9, 'rec.sport.baseball'),\n",
       " (10, 'rec.sport.hockey'),\n",
       " (11, 'sci.crypt'),\n",
       " (12, 'sci.electronics'),\n",
       " (13, 'sci.med'),\n",
       " (14, 'sci.space'),\n",
       " (15, 'soc.religion.christian'),\n",
       " (16, 'talk.politics.guns'),\n",
       " (17, 'talk.politics.mideast'),\n",
       " (18, 'talk.politics.misc'),\n",
       " (19, 'talk.religion.misc')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(idx, val) for idx, val in enumerate(twenty_train.target_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = \"GoogleNews-vectors-negative300.bin\"\n",
    "category_index = {}\n",
    "for idx, val in enumerate(twenty_train.target_names):\n",
    "    category_index[idx] = val \n",
    "category_reverse_index = dict((y,x) for (x,y) in category_index.items())\n",
    "STOPWORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def preprocess(text):\n",
    "    text = text.strip().lower().split()\n",
    "    text = filter(lambda word: word not in STOPWORDS, text)\n",
    "    return \" \".join(text)\n",
    "\n",
    "def create_dict(arr): # ramdom name - 'title'\n",
    "    return {'title': arr}\n",
    "\n",
    "train_df = pd.DataFrame(create_dict(twenty_train.data))\n",
    "test_df = pd.DataFrame(create_dict(twenty_test.data))\n",
    "\n",
    "dataset = [train_df, test_df]\n",
    "\n",
    "for data in dataset:\n",
    "    data['title'] = data['title'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_texts = train_df['title'] # + ' ' + test_df['title'] # to include the ' ' between ending of last and new statment.\n",
    "all_texts = all_texts.drop_duplicates(keep=False)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['title'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['title'])\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQ_LEN)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: \t (11314, 256)\n",
      "Test data: \t (7532, 256)\n",
      "--------------------\n",
      "Labels:\t (11314, 20)\n"
     ]
    }
   ],
   "source": [
    "print('Train data: \\t', train_data.shape)\n",
    "print('Test data: \\t', test_data.shape)\n",
    "print('-'*20)\n",
    "category = twenty_train.target\n",
    "category = to_categorical(category)\n",
    "print('Labels:\\t', category.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train - test split\n",
    "y_test_cat = to_categorical(twenty_test.target)\n",
    "X_train = train_data\n",
    "y_train = category\n",
    "X_test = test_data\n",
    "y_test = y_test_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL word embedding: 91382\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(MX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "        \n",
    "print('NULL word embedding: %d'% np.sum(np.sum(embedding_matrix, axis = 1) == 0))\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1\n",
    "                            embedding_matrix.shape[1], # or EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQ_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 256, 300)          40232400  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256, 256)          570368    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                5140      \n",
      "=================================================================\n",
      "Total params: 41,333,220\n",
      "Trainable params: 1,100,820\n",
      "Non-trainable params: 40,232,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import  Dense, Activation, LSTM, Input\n",
    "\n",
    "# Input Size\n",
    "sequence_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "# Input layer\n",
    "embedding_sequence = embedding_layer(sequence_input)\n",
    "# Hidden layer - 1\n",
    "x = LSTM(256, activation = 'tanh', return_sequences=True)(embedding_sequence)\n",
    "# Hidden layer - 2\n",
    "x = LSTM(256, activation = 'tanh')(x)\n",
    "# Output layer\n",
    "preds = Dense(20, activation = 'softmax')(x)\n",
    "# Loss function \n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Fit the model - Basic\n",
    "# model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "#           epochs=20, batch_size=128)\n",
    "# score = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Test loss: \\t', score[0])\n",
    "# print('Test Accuracy: \\t', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/30\n",
      "11314/11314 [==============================] - 766s 68ms/step - loss: 2.5698 - acc: 0.1682 - val_loss: 2.2001 - val_acc: 0.2475\n",
      "Epoch 2/30\n",
      "11314/11314 [==============================] - 753s 67ms/step - loss: 1.9881 - acc: 0.3047 - val_loss: 1.7276 - val_acc: 0.3877\n",
      "Epoch 3/30\n",
      "11314/11314 [==============================] - 792s 70ms/step - loss: 1.6302 - acc: 0.4252 - val_loss: 2.0204 - val_acc: 0.3524\n",
      "Epoch 4/30\n",
      "11314/11314 [==============================] - 873s 77ms/step - loss: 1.4520 - acc: 0.4902 - val_loss: 1.4553 - val_acc: 0.4679\n",
      "Epoch 5/30\n",
      "11314/11314 [==============================] - 841s 74ms/step - loss: 1.3131 - acc: 0.5293 - val_loss: 1.4392 - val_acc: 0.4708\n",
      "Epoch 6/30\n",
      "11314/11314 [==============================] - 846s 75ms/step - loss: 1.1928 - acc: 0.5774 - val_loss: 1.3614 - val_acc: 0.5364\n",
      "Epoch 7/30\n",
      "11314/11314 [==============================] - 841s 74ms/step - loss: 1.1123 - acc: 0.6054 - val_loss: 1.2812 - val_acc: 0.5717\n",
      "Epoch 8/30\n",
      "11314/11314 [==============================] - 843s 75ms/step - loss: 1.0064 - acc: 0.6515 - val_loss: 1.3027 - val_acc: 0.5754\n",
      "Epoch 9/30\n",
      "11314/11314 [==============================] - 819s 72ms/step - loss: 0.9335 - acc: 0.6791 - val_loss: 1.2358 - val_acc: 0.6000\n",
      "Epoch 10/30\n",
      "11314/11314 [==============================] - 848s 75ms/step - loss: 0.8663 - acc: 0.7036 - val_loss: 1.1674 - val_acc: 0.6228\n",
      "Epoch 11/30\n",
      "11314/11314 [==============================] - 828s 73ms/step - loss: 0.7956 - acc: 0.7305 - val_loss: 1.1796 - val_acc: 0.6345\n",
      "Epoch 12/30\n",
      "11314/11314 [==============================] - 846s 75ms/step - loss: 0.7602 - acc: 0.7409 - val_loss: 1.1206 - val_acc: 0.6479\n",
      "Epoch 13/30\n",
      "11314/11314 [==============================] - 847s 75ms/step - loss: 0.7081 - acc: 0.7608 - val_loss: 1.0973 - val_acc: 0.6606\n",
      "Epoch 14/30\n",
      "11314/11314 [==============================] - 754s 67ms/step - loss: 0.6679 - acc: 0.7735 - val_loss: 1.0913 - val_acc: 0.6560\n",
      "Epoch 15/30\n",
      "11314/11314 [==============================] - 731s 65ms/step - loss: 0.6250 - acc: 0.7914 - val_loss: 1.0636 - val_acc: 0.6743\n",
      "Epoch 16/30\n",
      "11314/11314 [==============================] - 792s 70ms/step - loss: 0.5936 - acc: 0.8008 - val_loss: 1.0755 - val_acc: 0.6676\n",
      "Epoch 17/30\n",
      "11314/11314 [==============================] - 836s 74ms/step - loss: 0.5669 - acc: 0.8127 - val_loss: 1.0673 - val_acc: 0.6787\n",
      "Epoch 18/30\n",
      "11314/11314 [==============================] - 849s 75ms/step - loss: 0.5399 - acc: 0.8197 - val_loss: 1.0811 - val_acc: 0.6790\n",
      "Epoch 19/30\n",
      "11314/11314 [==============================] - 850s 75ms/step - loss: 0.5153 - acc: 0.8282 - val_loss: 1.0416 - val_acc: 0.6893\n",
      "Epoch 20/30\n",
      "11314/11314 [==============================] - 837s 74ms/step - loss: 0.4887 - acc: 0.8380 - val_loss: 1.0568 - val_acc: 0.6907\n",
      "Epoch 21/30\n",
      "11314/11314 [==============================] - 850s 75ms/step - loss: 0.4643 - acc: 0.8464 - val_loss: 1.0923 - val_acc: 0.6782\n",
      "Epoch 22/30\n",
      "11314/11314 [==============================] - 839s 74ms/step - loss: 0.4511 - acc: 0.8525 - val_loss: 1.0487 - val_acc: 0.6965\n",
      "Epoch 23/30\n",
      "11314/11314 [==============================] - 744s 66ms/step - loss: 0.4238 - acc: 0.8616 - val_loss: 1.0469 - val_acc: 0.7019\n",
      "Epoch 24/30\n",
      "11314/11314 [==============================] - 725s 64ms/step - loss: 0.4074 - acc: 0.8663 - val_loss: 1.0556 - val_acc: 0.6972\n",
      "Test loss: \t 1.0555665914010823\n",
      "Test Accuracy: \t 0.6971587891978782\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Training the model and saving the best one!!\n",
    "batch_size = 128\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n",
    "earlystop = EarlyStopping(patience=5)\n",
    "modelsave = ModelCheckpoint(\n",
    "    filepath='rnn_model_try_no_2.h5', save_best_only=True, verbose=0)\n",
    "model.fit(\n",
    "    X_train, y_train, batch_size=batch_size,\n",
    "    epochs=30, \n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[annealer, earlystop, modelsave]\n",
    ")\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss: \\t', score[0])\n",
    "print('Test Accuracy: \\t', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # CNN Model - irrelevant but hey it works !! :D\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout, Activation\n",
    "# model = Sequential()\n",
    "# model.add(embedding_layer)\n",
    "# model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(250))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(20))\n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer = 'rmsprop', metrics=['acc'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=128)\n",
    "# score = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Test loss: \\t', score[0])\n",
    "# print('Test Accuracy: \\t', score[1])\n",
    "\n",
    "# model.save('my_model_20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_1 = Sequential()\n",
    "# model_1.add(embedding_layer)\n",
    "# model_1.add(Dropout(0.2))\n",
    "# model_1.add(Conv1D(512, 3, padding='same',activation='relu',strides=1))\n",
    "# model_1.add(Conv1D(256, 3, padding='same',activation='relu',strides=1))\n",
    "# model_1.add(Conv1D(128, 3, padding='same',activation='relu',strides=1))\n",
    "# model_1.add(Flatten())\n",
    "# model_1.add(Dropout(0.2))\n",
    "# model_1.add(Dense(150,activation='sigmoid'))\n",
    "# model_1.add(Dropout(0.2))\n",
    "# model_1.add(Dense(20,activation='sigmoid'))\n",
    "\n",
    "# model_1.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "\n",
    "# model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=128)\n",
    "# score = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Test loss: \\t', score[0])\n",
    "# print('Test Accuracy: \\t', score[1])\n",
    "\n",
    "# model.save('my_model_1_20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'alt.atheism', 1: 'comp.graphics', 2: 'comp.os.ms-windows.misc', 3: 'comp.sys.ibm.pc.hardware', 4: 'comp.sys.mac.hardware', 5: 'comp.windows.x', 6: 'misc.forsale', 7: 'rec.autos', 8: 'rec.motorcycles', 9: 'rec.sport.baseball', 10: 'rec.sport.hockey', 11: 'sci.crypt', 12: 'sci.electronics', 13: 'sci.med', 14: 'sci.space', 15: 'soc.religion.christian', 16: 'talk.politics.guns', 17: 'talk.politics.mideast', 18: 'talk.politics.misc', 19: 'talk.religion.misc'}\n"
     ]
    }
   ],
   "source": [
    "print(category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category:  sci.space\n"
     ]
    }
   ],
   "source": [
    "example_prediction = open('real_test.txt', 'r').read()\n",
    "example_prediction = preprocess(example_prediction)\n",
    "example_sequence = tokenizer.texts_to_sequences([example_prediction])\n",
    "example_padded_sequence = pad_sequences(example_sequence, maxlen=MAX_SEQ_LEN)\n",
    "# print(\"Predicted category: \", category_index[model.predict_classes(example_padded_sequence, verbose=0)[0]])\n",
    "y_pred = model.predict(example_padded_sequence)\n",
    "cat_predicted = np.argmax(y_pred,axis=1)\n",
    "\n",
    "print(\"Predicted category: \", category_index[cat_predicted[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
