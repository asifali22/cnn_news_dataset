{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Warning: 1.5 GB download.\n",
    "Download the bag of words (from Google News) from this link: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monster/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MX_NB_WORDS = 200000\n",
    "MAX_SEQ_LEN = 30\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: \t From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------------------------------------\n",
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peek on the data\n",
    "print('Data: \\t', \"\\n---------------------------------------------------\\n\".join(twenty_train.data[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'alt.atheism'),\n",
       " (1, 'comp.graphics'),\n",
       " (2, 'comp.os.ms-windows.misc'),\n",
       " (3, 'comp.sys.ibm.pc.hardware'),\n",
       " (4, 'comp.sys.mac.hardware'),\n",
       " (5, 'comp.windows.x'),\n",
       " (6, 'misc.forsale'),\n",
       " (7, 'rec.autos'),\n",
       " (8, 'rec.motorcycles'),\n",
       " (9, 'rec.sport.baseball'),\n",
       " (10, 'rec.sport.hockey'),\n",
       " (11, 'sci.crypt'),\n",
       " (12, 'sci.electronics'),\n",
       " (13, 'sci.med'),\n",
       " (14, 'sci.space'),\n",
       " (15, 'soc.religion.christian'),\n",
       " (16, 'talk.politics.guns'),\n",
       " (17, 'talk.politics.mideast'),\n",
       " (18, 'talk.politics.misc'),\n",
       " (19, 'talk.religion.misc')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(idx, val) for idx, val in enumerate(twenty_train.target_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = \"GoogleNews-vectors-negative300.bin\"\n",
    "category_index = {}\n",
    "for idx, val in enumerate(twenty_train.target_names):\n",
    "    category_index[idx] = val \n",
    "category_reverse_index = dict((y,x) for (x,y) in category_index.items())\n",
    "STOPWORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def preprocess(text):\n",
    "    text = text.strip().lower().split()\n",
    "    text = filter(lambda word: word not in STOPWORDS, text)\n",
    "    return \" \".join(text)\n",
    "\n",
    "def create_dict(arr): # ramdom name - 'title'\n",
    "    return {'title': arr}\n",
    "\n",
    "train_df = pd.DataFrame(create_dict(twenty_train.data))\n",
    "test_df = pd.DataFrame(create_dict(twenty_test.data))\n",
    "\n",
    "dataset = [train_df, test_df]\n",
    "\n",
    "for data in dataset:\n",
    "    data['title'] = data['title'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_texts = train_df['title'] + ' ' + test_df['title'] # ti include the ' ' between ending of last and new statment.\n",
    "all_texts = all_texts.drop_duplicates(keep=False)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['title'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['title'])\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQ_LEN)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: \t (11314, 30)\n",
      "Test data: \t (7532, 30)\n",
      "--------------------\n",
      "Labels:\t (11314, 20)\n"
     ]
    }
   ],
   "source": [
    "print('Train data: \\t', train_data.shape)\n",
    "print('Test data: \\t', test_data.shape)\n",
    "print('-'*20)\n",
    "category = twenty_train.target\n",
    "category = to_categorical(category)\n",
    "print('Labels:\\t', category.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train - test split\n",
    "\n",
    "y_test_cat = to_categorical(twenty_test.target)\n",
    "\n",
    "X_train = train_data\n",
    "y_train = category\n",
    "X_test = test_data\n",
    "y_test = y_test_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL word embedding: 112568\n",
      "----------\n",
      "Embedding matrix shape: (159848, 300)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(MX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "        \n",
    "print('NULL word embedding: %d'% np.sum(np.sum(embedding_matrix, axis = 1) == 0))\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1\n",
    "                            embedding_matrix.shape[1], # or EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQ_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 300)           47954400  \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 28, 250)           225250    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 20)                5020      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 48,247,420\n",
      "Trainable params: 293,020\n",
      "Non-trainable params: 47,954,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout, Activation\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(250))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(20))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'rmsprop', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 1.4037 - acc: 0.5659 - val_loss: 1.5735 - val_acc: 0.5179\n",
      "Epoch 2/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 1.0970 - acc: 0.6666 - val_loss: 1.5406 - val_acc: 0.5402\n",
      "Epoch 3/20\n",
      "11314/11314 [==============================] - 16s 1ms/step - loss: 0.8512 - acc: 0.7423 - val_loss: 1.5214 - val_acc: 0.5546\n",
      "Epoch 4/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.6408 - acc: 0.8136 - val_loss: 1.5808 - val_acc: 0.5629\n",
      "Epoch 5/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.4837 - acc: 0.8639 - val_loss: 1.7152 - val_acc: 0.5288\n",
      "Epoch 6/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.3539 - acc: 0.9048 - val_loss: 1.6842 - val_acc: 0.5491\n",
      "Epoch 7/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.2627 - acc: 0.9342 - val_loss: 1.6571 - val_acc: 0.5786\n",
      "Epoch 8/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.1947 - acc: 0.9526 - val_loss: 1.6762 - val_acc: 0.5787\n",
      "Epoch 9/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.1583 - acc: 0.9626 - val_loss: 1.9582 - val_acc: 0.5512\n",
      "Epoch 10/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.1261 - acc: 0.9715 - val_loss: 1.8785 - val_acc: 0.5628\n",
      "Epoch 11/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.1044 - acc: 0.9761 - val_loss: 2.0235 - val_acc: 0.5621\n",
      "Epoch 12/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.0960 - acc: 0.9775 - val_loss: 1.9911 - val_acc: 0.5795\n",
      "Epoch 13/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.0816 - acc: 0.9807 - val_loss: 2.0708 - val_acc: 0.5693\n",
      "Epoch 14/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.0697 - acc: 0.9819 - val_loss: 2.0975 - val_acc: 0.5730\n",
      "Epoch 15/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.0644 - acc: 0.9805 - val_loss: 2.1443 - val_acc: 0.5818\n",
      "Epoch 16/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.0544 - acc: 0.9836 - val_loss: 2.2901 - val_acc: 0.5758\n",
      "Epoch 17/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.0514 - acc: 0.9849 - val_loss: 2.2883 - val_acc: 0.5696\n",
      "Epoch 18/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.0477 - acc: 0.9836 - val_loss: 2.3579 - val_acc: 0.5765\n",
      "Epoch 19/20\n",
      "11314/11314 [==============================] - 17s 2ms/step - loss: 0.0439 - acc: 0.9844 - val_loss: 2.4622 - val_acc: 0.5617\n",
      "Epoch 20/20\n",
      "11314/11314 [==============================] - 17s 1ms/step - loss: 0.0419 - acc: 0.9847 - val_loss: 2.6498 - val_acc: 0.5542\n",
      "Test loss: \t 2.6497940551202728\n",
      "Test Accuracy: \t 0.5541688794476899\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=128)\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss: \\t', score[0])\n",
    "print('Test Accuracy: \\t', score[1])\n",
    "\n",
    "model.save('my_model_20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 300)           47954400  \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 30, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 30, 512)           461312    \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 30, 256)           393472    \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 30, 128)           98432     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 150)               576150    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 20)                3020      \n",
      "=================================================================\n",
      "Total params: 49,486,786\n",
      "Trainable params: 1,532,386\n",
      "Non-trainable params: 47,954,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(embedding_layer)\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Conv1D(512, 3, padding='same',activation='relu',strides=1))\n",
    "model_1.add(Conv1D(256, 3, padding='same',activation='relu',strides=1))\n",
    "model_1.add(Conv1D(128, 3, padding='same',activation='relu',strides=1))\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Dense(150,activation='sigmoid'))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Dense(20,activation='sigmoid'))\n",
    "\n",
    "model_1.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/20\n",
      "11314/11314 [==============================] - 74s 7ms/step - loss: 2.7674 - acc: 0.1124 - val_loss: 2.5050 - val_acc: 0.1888\n",
      "Epoch 2/20\n",
      "11314/11314 [==============================] - 73s 6ms/step - loss: 2.3097 - acc: 0.2223 - val_loss: 2.1660 - val_acc: 0.2714\n",
      "Epoch 3/20\n",
      "11314/11314 [==============================] - 73s 6ms/step - loss: 2.0281 - acc: 0.3163 - val_loss: 1.9537 - val_acc: 0.3496\n",
      "Epoch 4/20\n",
      "11314/11314 [==============================] - 74s 7ms/step - loss: 1.7548 - acc: 0.4075 - val_loss: 1.7991 - val_acc: 0.4057\n",
      "Epoch 5/20\n",
      "11314/11314 [==============================] - 73s 6ms/step - loss: 1.5215 - acc: 0.4898 - val_loss: 1.7246 - val_acc: 0.4399\n",
      "Epoch 6/20\n",
      "11314/11314 [==============================] - 73s 6ms/step - loss: 1.3188 - acc: 0.5547 - val_loss: 1.6871 - val_acc: 0.4632\n",
      "Epoch 7/20\n",
      "11314/11314 [==============================] - 78s 7ms/step - loss: 1.1283 - acc: 0.6199 - val_loss: 1.6087 - val_acc: 0.4887\n",
      "Epoch 8/20\n",
      "11314/11314 [==============================] - 74s 7ms/step - loss: 0.9548 - acc: 0.6786 - val_loss: 1.5945 - val_acc: 0.5198\n",
      "Epoch 9/20\n",
      "11314/11314 [==============================] - 73s 6ms/step - loss: 0.8008 - acc: 0.7399 - val_loss: 1.5830 - val_acc: 0.5345\n",
      "Epoch 10/20\n",
      "11314/11314 [==============================] - 73s 6ms/step - loss: 0.6667 - acc: 0.7876 - val_loss: 1.6190 - val_acc: 0.5385\n",
      "Epoch 11/20\n",
      "11314/11314 [==============================] - 73s 6ms/step - loss: 0.5358 - acc: 0.8358 - val_loss: 1.6930 - val_acc: 0.5311\n",
      "Epoch 12/20\n",
      "11314/11314 [==============================] - 73s 6ms/step - loss: 0.4460 - acc: 0.8644 - val_loss: 1.6890 - val_acc: 0.5471\n",
      "Epoch 13/20\n",
      "11314/11314 [==============================] - 73s 6ms/step - loss: 0.3728 - acc: 0.8897 - val_loss: 1.7348 - val_acc: 0.5426\n",
      "Epoch 14/20\n",
      "11314/11314 [==============================] - 73s 6ms/step - loss: 0.2921 - acc: 0.9156 - val_loss: 1.7600 - val_acc: 0.5510\n",
      "Epoch 15/20\n",
      "11314/11314 [==============================] - 76s 7ms/step - loss: 0.2469 - acc: 0.9313 - val_loss: 1.7934 - val_acc: 0.5546\n",
      "Epoch 16/20\n",
      "11314/11314 [==============================] - 74s 7ms/step - loss: 0.2086 - acc: 0.9420 - val_loss: 1.8642 - val_acc: 0.5518\n",
      "Epoch 17/20\n",
      "11314/11314 [==============================] - 81s 7ms/step - loss: 0.1880 - acc: 0.9486 - val_loss: 1.8664 - val_acc: 0.5453\n",
      "Epoch 18/20\n",
      "11314/11314 [==============================] - 74s 7ms/step - loss: 0.1596 - acc: 0.9587 - val_loss: 1.9321 - val_acc: 0.5501\n",
      "Epoch 19/20\n",
      "11314/11314 [==============================] - 73s 6ms/step - loss: 0.1302 - acc: 0.9665 - val_loss: 1.9293 - val_acc: 0.5562\n",
      "Epoch 20/20\n",
      "11314/11314 [==============================] - 74s 7ms/step - loss: 0.1286 - acc: 0.9640 - val_loss: 1.9717 - val_acc: 0.5554\n",
      "Test loss: \t 2.6497940551202728\n",
      "Test Accuracy: \t 0.5541688794476899\n"
     ]
    }
   ],
   "source": [
    "model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=128)\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss: \\t', score[0])\n",
    "print('Test Accuracy: \\t', score[1])\n",
    "\n",
    "model.save('my_model_1_20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'alt.atheism', 1: 'comp.graphics', 2: 'comp.os.ms-windows.misc', 3: 'comp.sys.ibm.pc.hardware', 4: 'comp.sys.mac.hardware', 5: 'comp.windows.x', 6: 'misc.forsale', 7: 'rec.autos', 8: 'rec.motorcycles', 9: 'rec.sport.baseball', 10: 'rec.sport.hockey', 11: 'sci.crypt', 12: 'sci.electronics', 13: 'sci.med', 14: 'sci.space', 15: 'soc.religion.christian', 16: 'talk.politics.guns', 17: 'talk.politics.mideast', 18: 'talk.politics.misc', 19: 'talk.religion.misc'}\n"
     ]
    }
   ],
   "source": [
    "print(category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category:  sci.space\n"
     ]
    }
   ],
   "source": [
    "example_prediction = open('real_test.txt', 'r').read()\n",
    "example_prediction = preprocess(example_prediction)\n",
    "example_sequence = tokenizer.texts_to_sequences([example_prediction])\n",
    "example_padded_sequence = pad_sequences(example_sequence, maxlen=MAX_SEQ_LEN)\n",
    "print(\"Predicted category: \", category_index[model_1.predict_classes(example_padded_sequence, verbose=0)[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
