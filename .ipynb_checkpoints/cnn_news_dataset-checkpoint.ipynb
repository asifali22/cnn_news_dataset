{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWarning: 1.5 GB download.\\nDownload the bag of words (from Google News) from this link: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Warning: 1.5 GB download.\n",
    "Download the bag of words (from Google News) from this link: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monster/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MX_NB_WORDS = 200000\n",
    "MAX_SEQ_LEN = 100\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: \t From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------------------------------------\n",
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peek on the data\n",
    "print('Data: \\t', \"\\n---------------------------------------------------\\n\".join(twenty_train.data[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'alt.atheism'),\n",
       " (1, 'comp.graphics'),\n",
       " (2, 'comp.os.ms-windows.misc'),\n",
       " (3, 'comp.sys.ibm.pc.hardware'),\n",
       " (4, 'comp.sys.mac.hardware'),\n",
       " (5, 'comp.windows.x'),\n",
       " (6, 'misc.forsale'),\n",
       " (7, 'rec.autos'),\n",
       " (8, 'rec.motorcycles'),\n",
       " (9, 'rec.sport.baseball'),\n",
       " (10, 'rec.sport.hockey'),\n",
       " (11, 'sci.crypt'),\n",
       " (12, 'sci.electronics'),\n",
       " (13, 'sci.med'),\n",
       " (14, 'sci.space'),\n",
       " (15, 'soc.religion.christian'),\n",
       " (16, 'talk.politics.guns'),\n",
       " (17, 'talk.politics.mideast'),\n",
       " (18, 'talk.politics.misc'),\n",
       " (19, 'talk.religion.misc')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(idx, val) for idx, val in enumerate(twenty_train.target_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = \"GoogleNews-vectors-negative300.bin\"\n",
    "category_index = {}\n",
    "for idx, val in enumerate(twenty_train.target_names):\n",
    "    category_index[idx] = val \n",
    "category_reverse_index = dict((y,x) for (x,y) in category_index.items())\n",
    "STOPWORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def preprocess(text):\n",
    "    text = text.strip().lower().split()\n",
    "    text = filter(lambda word: word not in STOPWORDS, text)\n",
    "    return \" \".join(text)\n",
    "\n",
    "def create_dict(arr): # ramdom name - 'title'\n",
    "    return {'title': arr}\n",
    "\n",
    "train_df = pd.DataFrame(create_dict(twenty_train.data))\n",
    "test_df = pd.DataFrame(create_dict(twenty_test.data))\n",
    "\n",
    "dataset = [train_df, test_df]\n",
    "\n",
    "for data in dataset:\n",
    "    data['title'] = data['title'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = train_df['title'] # + ' ' + test_df['title'] # to include the ' ' between ending of last and new statment.\n",
    "all_texts = all_texts.drop_duplicates(keep=False)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['title'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['title'])\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQ_LEN)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: \t (11314, 100)\n",
      "Test data: \t (7532, 100)\n",
      "--------------------\n",
      "Labels:\t (11314, 20)\n"
     ]
    }
   ],
   "source": [
    "print('Train data: \\t', train_data.shape)\n",
    "print('Test data: \\t', test_data.shape)\n",
    "print('-'*20)\n",
    "category = twenty_train.target\n",
    "category = to_categorical(category)\n",
    "print('Labels:\\t', category.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train - test split\n",
    "y_test_cat = to_categorical(twenty_test.target)\n",
    "X_train = train_data\n",
    "y_train = category\n",
    "X_test = test_data\n",
    "y_test = y_test_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL word embedding: 91382\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(MX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "        \n",
    "print('NULL word embedding: %d'% np.sum(np.sum(embedding_matrix, axis = 1) == 0))\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1\n",
    "                            embedding_matrix.shape[1], # or EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQ_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 300)          40232400  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               570368    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                5140      \n",
      "=================================================================\n",
      "Total params: 40,807,908\n",
      "Trainable params: 575,508\n",
      "Non-trainable params: 40,232,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import  Dense, Activation, LSTM, Input\n",
    "\n",
    "# Input Size\n",
    "sequence_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "# Input layer\n",
    "embedding_sequence = embedding_layer(sequence_input)\n",
    "# Hidden layer - 1\n",
    "x = LSTM(256, activation = 'tanh')(embedding_sequence)\n",
    "# Output layer\n",
    "preds = Dense(20, activation = 'softmax')(x)\n",
    "# Loss function \n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Fit the model - Basic\n",
    "# model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "#           epochs=20, batch_size=128)\n",
    "# score = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Test loss: \\t', score[0])\n",
    "# print('Test Accuracy: \\t', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/30\n",
      "11314/11314 [==============================] - 163s 14ms/step - loss: 2.7177 - acc: 0.1462 - val_loss: 2.5379 - val_acc: 0.1980\n",
      "Epoch 2/30\n",
      "11314/11314 [==============================] - 156s 14ms/step - loss: 2.3009 - acc: 0.2571 - val_loss: 2.2996 - val_acc: 0.2403\n",
      "Epoch 3/30\n",
      "11314/11314 [==============================] - 151s 13ms/step - loss: 1.9994 - acc: 0.3420 - val_loss: 1.9967 - val_acc: 0.3323\n",
      "Epoch 4/30\n",
      "11314/11314 [==============================] - 149s 13ms/step - loss: 1.7354 - acc: 0.4248 - val_loss: 1.7572 - val_acc: 0.4156\n",
      "Epoch 5/30\n",
      "11314/11314 [==============================] - 149s 13ms/step - loss: 1.5646 - acc: 0.4864 - val_loss: 1.6016 - val_acc: 0.4677\n",
      "Epoch 6/30\n",
      "11314/11314 [==============================] - 150s 13ms/step - loss: 1.3580 - acc: 0.5475 - val_loss: 1.5367 - val_acc: 0.5004\n",
      "Epoch 7/30\n",
      "11314/11314 [==============================] - 151s 13ms/step - loss: 1.2609 - acc: 0.5796 - val_loss: 1.4101 - val_acc: 0.5408\n",
      "Epoch 8/30\n",
      "11314/11314 [==============================] - 150s 13ms/step - loss: 1.1533 - acc: 0.6192 - val_loss: 1.3462 - val_acc: 0.5620\n",
      "Epoch 9/30\n",
      "11314/11314 [==============================] - 148s 13ms/step - loss: 1.0762 - acc: 0.6450 - val_loss: 1.3438 - val_acc: 0.5791\n",
      "Epoch 10/30\n",
      "11314/11314 [==============================] - 148s 13ms/step - loss: 1.0011 - acc: 0.6736 - val_loss: 1.3181 - val_acc: 0.5705\n",
      "Epoch 11/30\n",
      "11314/11314 [==============================] - 151s 13ms/step - loss: 0.9267 - acc: 0.6965 - val_loss: 1.3051 - val_acc: 0.5832\n",
      "Epoch 12/30\n",
      "11314/11314 [==============================] - 149s 13ms/step - loss: 0.8857 - acc: 0.7088 - val_loss: 1.2286 - val_acc: 0.6113\n",
      "Epoch 13/30\n",
      "11314/11314 [==============================] - 151s 13ms/step - loss: 0.8317 - acc: 0.7299 - val_loss: 1.1835 - val_acc: 0.6241\n",
      "Epoch 14/30\n",
      "11314/11314 [==============================] - 147s 13ms/step - loss: 0.7876 - acc: 0.7420 - val_loss: 1.1614 - val_acc: 0.6287\n",
      "Epoch 15/30\n",
      "11314/11314 [==============================] - 148s 13ms/step - loss: 0.7442 - acc: 0.7553 - val_loss: 1.2133 - val_acc: 0.6271\n",
      "Epoch 16/30\n",
      "11314/11314 [==============================] - 148s 13ms/step - loss: 0.7133 - acc: 0.7679 - val_loss: 1.1377 - val_acc: 0.6577\n",
      "Epoch 17/30\n",
      "11314/11314 [==============================] - 149s 13ms/step - loss: 0.6750 - acc: 0.7782 - val_loss: 1.1373 - val_acc: 0.6539\n",
      "Epoch 18/30\n",
      "11314/11314 [==============================] - 148s 13ms/step - loss: 0.6503 - acc: 0.7857 - val_loss: 1.1796 - val_acc: 0.6387\n",
      "Epoch 19/30\n",
      "11314/11314 [==============================] - 145s 13ms/step - loss: 0.6175 - acc: 0.7972 - val_loss: 1.1763 - val_acc: 0.6476\n",
      "Epoch 20/30\n",
      "11314/11314 [==============================] - 144s 13ms/step - loss: 0.5929 - acc: 0.8099 - val_loss: 1.1502 - val_acc: 0.6560\n",
      "Epoch 21/30\n",
      "11314/11314 [==============================] - 143s 13ms/step - loss: 0.5710 - acc: 0.8132 - val_loss: 1.1398 - val_acc: 0.6616\n",
      "Epoch 22/30\n",
      "11314/11314 [==============================] - 141s 12ms/step - loss: 0.5465 - acc: 0.8212 - val_loss: 1.1645 - val_acc: 0.6652\n",
      "Test loss: \t 1.164506271995893\n",
      "Test Accuracy: \t 0.6651619755708975\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Training the model and saving the best one!!\n",
    "batch_size = 128\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n",
    "earlystop = EarlyStopping(patience=5)\n",
    "modelsave = ModelCheckpoint(\n",
    "    filepath='rnn_model_try_no_1.h5', save_best_only=True, verbose=0)\n",
    "model.fit(\n",
    "    X_train, y_train, batch_size=batch_size,\n",
    "    epochs=30, \n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[annealer, earlystop, modelsave]\n",
    ")\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss: \\t', score[0])\n",
    "print('Test Accuracy: \\t', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # CNN Model - irrelevant but hey it works !! :D\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout, Activation\n",
    "# model = Sequential()\n",
    "# model.add(embedding_layer)\n",
    "# model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(250))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(20))\n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer = 'rmsprop', metrics=['acc'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=128)\n",
    "# score = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Test loss: \\t', score[0])\n",
    "# print('Test Accuracy: \\t', score[1])\n",
    "\n",
    "# model.save('my_model_20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_1 = Sequential()\n",
    "# model_1.add(embedding_layer)\n",
    "# model_1.add(Dropout(0.2))\n",
    "# model_1.add(Conv1D(512, 3, padding='same',activation='relu',strides=1))\n",
    "# model_1.add(Conv1D(256, 3, padding='same',activation='relu',strides=1))\n",
    "# model_1.add(Conv1D(128, 3, padding='same',activation='relu',strides=1))\n",
    "# model_1.add(Flatten())\n",
    "# model_1.add(Dropout(0.2))\n",
    "# model_1.add(Dense(150,activation='sigmoid'))\n",
    "# model_1.add(Dropout(0.2))\n",
    "# model_1.add(Dense(20,activation='sigmoid'))\n",
    "\n",
    "# model_1.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "\n",
    "# model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=128)\n",
    "# score = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Test loss: \\t', score[0])\n",
    "# print('Test Accuracy: \\t', score[1])\n",
    "\n",
    "# model.save('my_model_1_20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'alt.atheism', 1: 'comp.graphics', 2: 'comp.os.ms-windows.misc', 3: 'comp.sys.ibm.pc.hardware', 4: 'comp.sys.mac.hardware', 5: 'comp.windows.x', 6: 'misc.forsale', 7: 'rec.autos', 8: 'rec.motorcycles', 9: 'rec.sport.baseball', 10: 'rec.sport.hockey', 11: 'sci.crypt', 12: 'sci.electronics', 13: 'sci.med', 14: 'sci.space', 15: 'soc.religion.christian', 16: 'talk.politics.guns', 17: 'talk.politics.mideast', 18: 'talk.politics.misc', 19: 'talk.religion.misc'}\n"
     ]
    }
   ],
   "source": [
    "print(category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category:  sci.space\n"
     ]
    }
   ],
   "source": [
    "example_prediction = open('real_test.txt', 'r').read()\n",
    "example_prediction = preprocess(example_prediction)\n",
    "example_sequence = tokenizer.texts_to_sequences([example_prediction])\n",
    "example_padded_sequence = pad_sequences(example_sequence, maxlen=MAX_SEQ_LEN)\n",
    "# print(\"Predicted category: \", category_index[model.predict_classes(example_padded_sequence, verbose=0)[0]])\n",
    "y_pred = model.predict(example_padded_sequence)\n",
    "cat_predicted = np.argmax(y_pred,axis=1)\n",
    "\n",
    "print(\"Predicted category: \", category_index[cat_predicted[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
